# -*- coding: utf-8 -*-
"""평점+XAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z6MIic983qighv_eHeLVwRB8crztUmGM
"""
"""# TF 구현"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import urllib.request
from konlpy.tag import Okt
from tqdm import tqdm
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.prepipprocessing.sequence import pad_sequences
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt", filename="ratings_train.txt")
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="ratings_test.txt")

train_data = pd.read_table('./ratings_train.txt')
test_data = pd.read_table('./ratings_test.txt')

train_data.drop_duplicates(subset=['document'], inplace=True)

train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거
print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인

train_data

train_data['document'] = train_data['document'].str.replace('^ +', "") # white space 데이터를 empty value로 변경
train_data['document'].replace('', np.nan, inplace=True)
train_data = train_data.dropna(how = 'any')

test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거
test_data['document'] = test_data['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","") # 정규 표현식 수행
test_data['document'] = test_data['document'].str.replace('^ +', "") # 공백은 empty 값으로 변경
test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경
test_data = test_data.dropna(how='any') # Null 값 제거
print('전처리 후 테스트용 샘플의 개수 :',len(test_data))

okt = Okt()
okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)

stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']

X_train = []
for sentence in tqdm(train_data['document']):
    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    status = ""
    for i in stopwords_removed_sentence:
      status += i + " "
    X_train.append(status)

X_test = []
for sentence in tqdm(test_data['document']):
    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    status = ""
    for i in stopwords_removed_sentence:
      status += i + " "
    X_test.append(status)

X_train = np.array(X_train)
X_test = np.array(X_test)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

threshold = 3
total_cnt = len(tokenizer.word_index) # 단어의 수
rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트
total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합
rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합

# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # 단어의 등장 빈도수가 threshold보다 작으면
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('단어 집합(vocabulary)의 크기 :',total_cnt)
print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))
print("단어 집합에서 희귀 단어의 비율:", (rare_cnt / total_cnt)*100)
print("전체 등장 빈도에서 희귀 단어 등장 빈도 비율:", (rare_freq / total_freq)*100)

# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.
# 0번 패딩 토큰을 고려하여 + 1
vocab_size = total_cnt - rare_cnt + 1
print('단어 집합의 크기 :',vocab_size)

drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]

X_train = np.delete(X_train, drop_train, axis=0)
print(len(X_train))

tokenizer = Tokenizer(vocab_size) 
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

max_len = 50

X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

X_train

text = ["와 영화 개노잼이네 진짜 망할듯",
        "시간 가는 줄 모르고 재밌게 봤습니다.",
        "ㄹㅇ 개노잼"]

text_processing = []
for i in text:
    tokenized_sentence = okt.morphs(i, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    status = ""
    for i in stopwords_removed_sentence:
      status += i + " "
    text_processing.append(status)

text_processing = tokenizer.texts_to_sequences(text_processing)

X_test[:50]

test_size = 50
text_score = [0 for i in range(test_size)]

for i in tqdm(range(len(X_test[:test_size]))):
  for j in X_train:
    union = set(X_test[i]).union(set(j))
    intersection = set(X_test[i]).intersection(set(j))
    text_score[i] = max(text_score[i], len(intersection) / len(union))

test_data.head(30)

test_data_head = test_data.head(50)
test_data_head['score'] = text_score

test_data_head

text_score

"""# Doc2Vec"""

from gensim.models import doc2vec
from gensim.models.doc2vec import TaggedDocument
from konlpy.tag import Mecab

model = doc2vec.Doc2Vec(vector_size=vocab_size, alpha=0.025, min_alpha=0.025, workers=8, window=8)

# Vocabulary 빌드
model.build_vocab(tagged_corpus_list)
print(f"Tag Size: {len(model.docvecs.doctags.keys())}", end=' / ')

# Doc2Vec 학습
model.train(tagged_corpus_list, total_examples=model.corpus_count, epochs=50)

# 모델 저장
model.save('./dart.doc2vec')





"""# TF, TF-IDF -> 램 부족으로 유기"""

tfvector = CountVectorizer()
tfvector.fit(X_train)
# 코퍼스로부터 각 단어의 빈도수를 기록

# 각 단어와 맵핑된 인덱스 출력
# print(vector.vocabulary_)

tfvector.fit_transform(X_train)

tfidf = TfidfVectorizer()
tfidf.fit(X_train)
x_tfidf = tfidf.fit_transform(X_train)

len(X_train)

x_tfidf.A

tfidf.transform(X_train).toarray()

"""# 가짜 리뷰(비슷한 리뷰 자카드 유사도 활용) ㄹㅇ 프로토타입


"""

text = ["와 영화 개노잼이네 진짜 망할듯",
        "시간 가는 줄 모르고 재밌게 봤습니다.",
        "ㄹㅇ 개노잼"]

text_processing = []
for i in text:
    tokenized_sentence = okt.morphs(i, stem=True) # 토큰화
    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거
    status = ""
    for i in stopwords_removed_sentence:
      status += i + " "
    text_processing.append(status)

text_processing = tokenizer.texts_to_sequences(text_processing)

text_score = [0 for i in range(len(text))]

for i in range(len(text_processing)):
  for j in X_train:
    union = set(text_processing[i]).union(set(j))
    intersection = set(text_processing[i]).intersection(set(j))
    text_score[i] = max(text_score[i], len(intersection) / len(union))

text_score

"""# 리뷰 요약"""

import torch
from transformers import PreTrainedTokenizerFast
from transformers import BartForConditionalGeneration

tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')
model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')



